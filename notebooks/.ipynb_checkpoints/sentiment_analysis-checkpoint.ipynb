{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ac99a82-39fa-430f-8424-2d10b59ff554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# End to End Tweet Sentiment analysis using TFIDF\n",
    "In this notebook, we will be building a machine learning model to perform tweet sentiment analysis using pyspark. We will not be just training a simple model but we will also be exploring the use of advance functions in pyspark such as pipeline, gridsearch. Furthermore, we will me tracking our experiments using MLFOW and then deploying the model.\n",
    "\n",
    "\n",
    "### Data\n",
    "- We will be using Sentiment140 from standford http://help.sentiment140.com/for-students/\n",
    "- I have reduced the size by 8 times to reduce the training time\n",
    "\n",
    "### Data preparation\n",
    "- pyspark inbuild pipeline \n",
    "\n",
    "### Model training\n",
    "- pyspark inbuild crossvalidation\n",
    "- pyspark inbuild gridsearch\n",
    "- pyspark inbuild machine learning models\n",
    "\n",
    "### Model Tracking\n",
    "- MLFLOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08c1ff8d-803f-4b10-a002-dfe847a8add6",
     "showTitle": true,
     "title": "Importing libraries"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, countDistinct\n",
    "from pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegression\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.util import MLWriter\n",
    "\n",
    "\n",
    "#mlflow\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "\n",
    "#python\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    experiment_name='Sentiment_analysis_LR'\n",
    "    mlflow_uri = \"http://localhost:5000\"\n",
    "    raw_data_fp = os.path.join(\"..\",\"data\",\"sentiment_analysis.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting mlflow experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(CONFIG.experiment_name)\n",
    "mlflow.set_tracking_uri(CONFIG.mlflow_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(CONFIG.experiment_name).config(\"spark.driver.memory\", \"10g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "faeb9355-6776-44c8-af65-f9d8ba13863a",
     "showTitle": true,
     "title": "Loading data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+--------------------+--------+-------------+--------------------+\n",
      "|   _c0|  0|         1|                   2|       3|            4|                   5|\n",
      "+------+---+----------+--------------------+--------+-------------+--------------------+\n",
      "|779483|  0|2322939033|Wed Jun 24 23:34:...|NO_QUERY|Olivia_Hebert|right about now, ...|\n",
      "| 74208|  0|1694744393|Mon May 04 03:57:...|NO_QUERY|   MasqueArts|this rain wasn't ...|\n",
      "|310272|  0|2001072003|Mon Jun 01 23:45:...|NO_QUERY|     phlthy01|@madathena: K nev...|\n",
      "|290810|  0|1995500009|Mon Jun 01 13:35:...|NO_QUERY| FleaFletcher| there's so much ...|\n",
      "|710133|  0|2257822485|Sat Jun 20 14:57:...|NO_QUERY|bikerchick250|I wish I were at ...|\n",
      "+------+---+----------+--------------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(CONFIG.raw_data_fp,header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00fa34a1-cb59-480d-bd06-3806897b4f5d",
     "showTitle": true,
     "title": "Renaming the columns"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+--------------------+--------+-------------+--------------------+\n",
      "| index|polarity|        id|                date|   query|         user|                text|\n",
      "+------+--------+----------+--------------------+--------+-------------+--------------------+\n",
      "|779483|       0|2322939033|Wed Jun 24 23:34:...|NO_QUERY|Olivia_Hebert|right about now, ...|\n",
      "| 74208|       0|1694744393|Mon May 04 03:57:...|NO_QUERY|   MasqueArts|this rain wasn't ...|\n",
      "|310272|       0|2001072003|Mon Jun 01 23:45:...|NO_QUERY|     phlthy01|@madathena: K nev...|\n",
      "+------+--------+----------+--------------------+--------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_column_names = ['index','polarity', 'id', 'date', 'query', 'user', 'text']\n",
    "df = df.select([df[original].alias(new) for original, new in zip(df.columns,new_column_names)])\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf80ecb5-daff-4b63-aec8-00a0abf1174d",
     "showTitle": true,
     "title": "Checking Schema and casting if necessary"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- polarity: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42f272e5-5e71-46c5-ba4d-9315efb48ec0",
     "showTitle": true,
     "title": "Basic Statistics"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|summary|             index|          polarity|                  id|                date|   query|                user|                text|\n",
      "+-------+------------------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|  count|            200000|            200000|              200000|              200000|  200000|              200000|              200000|\n",
      "|   mean|      800244.80057|               2.0|  1.99898937403071E9|                null|    null|1.3452843115681818E9|                null|\n",
      "| stddev|462222.90411669045|2.0000050000187506|1.9375194913178074E8|                null|    null| 8.371814238571698E9|                null|\n",
      "|    min|                11|                 0|          1467812579|Fri Apr 17 20:30:...|NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|           1599987|                 4|          2329204835|Wed May 27 07:27:...|NO_QUERY|           zzzValzzz|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f23da62d-f169-4d1c-9705-267ed02a5f0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that we have 200k rows of tweets which is quite a huge number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b4d2438-c905-4976-b518-c6d9c2816a49",
     "showTitle": true,
     "title": "Check for missing values"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------+------------+-------------+------------+------------+\n",
      "|missing_index|missing_polarity|missing_id|missing_date|missing_query|missing_user|missing_text|\n",
      "+-------------+----------------+----------+------------+-------------+------------+------------+\n",
      "|            0|               0|         0|           0|            0|           0|           0|\n",
      "+-------------+----------------+----------+------------+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(col),col)).alias('missing_'+col) for col in df.columns]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0db04fee-77e4-45cb-859e-fbdb34cb6ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fortunately for us, we do not have any missing values. We can also see that for pyspark, checking of missing values is not as straight forward in pandas where we just  have to use isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5351ead-8d73-4ad0-849c-4dfe2e08933f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Value Counts\n",
    "Although we will be using only the tweets as an input to the model, we do want to do some sanity check on our dataset and this also important for continuous training to identify drift in the input data\n",
    "\n",
    "1. we can see that there are no neutral tweets, all of the tweets are either 0(negative) or 4(positive), we will change the polarity to [0,1] column and frame the problem statment into a binary classifier\n",
    "2. The dataset is very balanced with a equal split between the positive and negative tweets\n",
    "3. All of the columns are no query and therefore we can drop that column since there are no variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00477e0b-d388-4957-97e5-bee3d3b5395f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For column polarity\n",
      "+--------+------+\n",
      "|polarity| count|\n",
      "+--------+------+\n",
      "|       0|100000|\n",
      "|       4|100000|\n",
      "+--------+------+\n",
      "\n",
      "For column query\n",
      "+--------+------+\n",
      "|   query| count|\n",
      "+--------+------+\n",
      "|NO_QUERY|200000|\n",
      "+--------+------+\n",
      "\n",
      "For column user\n",
      "+---------------+-----+\n",
      "|           user|count|\n",
      "+---------------+-----+\n",
      "|       lost_dog|   62|\n",
      "|        webwoke|   45|\n",
      "|       tweetpet|   43|\n",
      "|SallytheShizzle|   40|\n",
      "|    VioletsCRUK|   40|\n",
      "|      Jayme1988|   39|\n",
      "|      DarkPiano|   35|\n",
      "|    mcraddictal|   35|\n",
      "|    Karen230683|   35|\n",
      "|    what_bugs_u|   34|\n",
      "|   SongoftheOss|   34|\n",
      "|         keza34|   34|\n",
      "|       tsarnick|   31|\n",
      "|     Spidersamm|   30|\n",
      "|          StDAY|   30|\n",
      "|   TraceyHewins|   29|\n",
      "|     SarahSaner|   29|\n",
      "|    Dutchrudder|   25|\n",
      "|        Dogbook|   25|\n",
      "|      shanajaca|   24|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in ['polarity','query','user']:\n",
    "    print(f\"For column {column}\")\n",
    "    df.select(column).groupby(column).agg(count(column).alias(\"count\")).orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b7d00d3-16ef-44f7-a490-5aff1c72291d",
     "showTitle": true,
     "title": "Converting the label into binary labels"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|100000|\n",
      "|    0|100000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('label', when(df['polarity']==0, 0).otherwise(1)).drop('polarity')\n",
    "column='label'\n",
    "df.select(column).groupby(column).agg(count(column).alias(\"count\")).orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "caffbeb7-271b-4f9a-ad09-78ce94132704",
     "showTitle": true,
     "title": "Number of unique user"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT user)|\n",
      "+--------------------+\n",
      "|              148659|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column = 'user'\n",
    "df.select(countDistinct(column)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f3eaacd-e27d-4e54-8ace-6d1d06b4eeee",
     "showTitle": true,
     "title": "Selecting the training features"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|right about now, ...|    0|\n",
      "|this rain wasn't ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_train_df = df.select(['text','label'])\n",
    "full_train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2501c2f1-fc36-4290-8672-26679c17031c",
     "showTitle": true,
     "title": "Remove duplicates as best practice"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+\n",
      "|summary|                text|            label|\n",
      "+-------+--------------------+-----------------+\n",
      "|  count|              199146|           199146|\n",
      "|   mean|                null| 0.50034145802577|\n",
      "| stddev|                null|0.500001138771227|\n",
      "|    min|                 ...|                0|\n",
      "|    max|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|                1|\n",
      "+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_train_df = full_train_df.dropDuplicates(['text'])\n",
    "full_train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c73ff49-bf6e-4113-ae37-dfaf6a1173aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For huge dataset like this, it is costly to combine the model into the pipeline and train in cross-validated grid search and therefore for this project the transformation and the model is seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9c5ee78-9cea-4e5e-a109-35f8b60273db",
     "showTitle": true,
     "title": "Transformation pipeline"
    }
   },
   "outputs": [],
   "source": [
    "max_ngram = 3 #trigram\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokenized')\n",
    "\n",
    "n_gram_pipe = [NGram(n=n, inputCol='tokenized', outputCol=f\"{n}_gram\")\n",
    "  for n in range(1, max_ngram+1) \n",
    "]\n",
    "hash_tf_pipe = [CountVectorizer(vocabSize=12000, inputCol=f\"{n}_gram\", outputCol=f\"{n}_gram_tf\")\n",
    "  for n in range(1, max_ngram+1)               \n",
    "]\n",
    "idf_pipe = [IDF(inputCol=f\"{n}_gram_tf\", outputCol=f\"{n}_gram_idf\", minDocFreq=5)\n",
    "  for n in range(1, max_ngram+1)\n",
    "]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=[f\"{n}_gram_idf\" for n in range(1, max_ngram+1)], outputCol='features')\n",
    "\n",
    "feature_transformation_pipeline = Pipeline(stages=[tokenizer, *n_gram_pipe, *hash_tf_pipe, *idf_pipe, vector_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6abbddf8-84cf-4587-86bf-6ec377170ae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fitted_pipeline = feature_transformation_pipeline.fit(full_train_df)\n",
    "transformed_df = fitted_pipeline.transform(full_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62a01150-c8ba-444d-86a4-4366ceb3b29e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = transformed_df.select(['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(42000,[2,62,362,...|    0|\n",
      "|(42000,[0,1,8,14,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97992508-8298-445b-bd3b-28101e906981",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter search\n",
    "One down side of pyspark ml is the lack of stratifiedKfold split. For this project, we will be using TrainValidationSplit instead due to the long training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/05 15:49:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 15:49:23 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 15:49:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 15:49:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58f40de6-d8e8-4591-b086-8de362b1389d",
     "showTitle": true,
     "title": "Modeling"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/05 15:49:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 15:49:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 15:49:35 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2022/01/05 16:34:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: An error occurred while calling o2125.saveAsTextFile.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:105)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1102.0 (TID 78822, DESKTOP-NQ52IDC, executor driver): java.io.IOException: Mkdirs failed to create file:/tmp/mlflow/32076571-0923-4b07-add6-c369434f9c8b/metadata/_temporary/0/_temporary/attempt_202201051634162646333063903371967_1758_m_000000_0 (exists=false, cwd=file:/C:/Users/teozz/OneDrive/Desktop/vscode_workspace/projects/py_spark_ml/notebooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:235)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:125)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2167)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 50 more\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/tmp/mlflow/32076571-0923-4b07-add6-c369434f9c8b/metadata/_temporary/0/_temporary/attempt_202201051634162646333063903371967_1758_m_000000_0 (exists=false, cwd=file:/C:/Users/teozz/OneDrive/Desktop/vscode_workspace/projects/py_spark_ml/notebooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:235)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:125)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/05 16:34:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Please create a new Spark session and ensure you have the mlflow-spark JAR attached to your Spark session as described in http://mlflow.org/docs/latest/tracking.html#automatic-logging-from-spark-experimental. Exception:\n",
      "'JavaPackage' object is not callable\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as active_run:\n",
    "    model = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "    param_grid = ParamGridBuilder().addGrid(model.elasticNetParam,[0.1,0.15]).addGrid(model.regParam,[0.05,0.1]).build()\n",
    "\n",
    "    train_valid_clf = TrainValidationSplit(estimator=model, \n",
    "                                           estimatorParamMaps=param_grid, \n",
    "                                           evaluator=BinaryClassificationEvaluator(labelCol='label'), \n",
    "                                           trainRatio=0.9)\n",
    "    trained_model = train_valid_clf.fit(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cc37eee-153f-459f-9801-42a00a4672a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8295957362589691, 0.7916592198633501, 0.8092940953845127, 0.7573227502238503]\n"
     ]
    }
   ],
   "source": [
    "print(trained_model.validationMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "sentiment_analysis",
   "notebookOrigID": 3421766781212103,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
